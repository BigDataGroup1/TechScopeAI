# Complete Setup Guide - TechScopeAI Data Downloader

## âœ… **What's Done**

1. âœ… **StackOverflow removed** from config (you have better job data)
2. âœ… **Skip logic added** - Won't re-download existing data
3. âœ… **All broken datasets fixed** in config
4. âœ… **New downloaders created** (Hacker News, RSS, Reddit, Article Scraper)

---

## ğŸš€ **How to Run**

### **Download All Data:**
```bash
./venv/bin/python scripts/download_datasets.py
```

### **Download Specific Categories:**
```bash
# Only competitive data
./venv/bin/python scripts/download_datasets.py --agents competitive

# Only pitch data
./venv/bin/python scripts/download_datasets.py --agents pitch

# Multiple categories
./venv/bin/python scripts/download_datasets.py --agents competitive pitch marketing
```

---

## â­ï¸ **Skip Logic (NEW!)**

The script now **automatically skips** data that already exists!

**What happens:**
- âœ… Checks if files/directories exist with content
- âœ… Skips download if data is already there
- âœ… Shows: `â­ï¸  Skipping {name} - data already exists`
- âœ… Safe to run multiple times!

**Example output:**
```
Downloading hacker-news-startups...
â­ï¸  Skipping hacker-news-startups - data already exists
âœ“ Successfully downloaded hacker-news-startups
```

---

## ğŸ“Š **Current Status**

### **Working Datasets (28+):**
- âœ… Hacker News stories
- âœ… RSS feeds (TechCrunch, VentureBeat)
- âœ… Kaggle datasets (with credentials)
- âœ… HuggingFace datasets
- âœ… GitHub repos
- âœ… Web scraping (YC blogs, investor blogs, etc.)

### **Needs Manual Setup:**
- âš ï¸ **AdImageNet** - Needs HuggingFace token (optional)
- âš ï¸ **Reddit** - Needs API credentials (optional, commented out)

### **Removed/Fixed:**
- âŒ StackOverflow - Removed (you have better job data)
- âŒ Deprecated HuggingFace datasets - Commented out
- âŒ Broken GitHub repos - Commented out
- âŒ Product Hunt - Commented out (blocks scrapers)

---

## ğŸ”§ **Optional Setup**

### **1. HuggingFace Token (for gated datasets):**
```bash
# Get token from: https://huggingface.co/settings/tokens
# Add to .env:
echo "HF_TOKEN=your_token_here" >> .env
```

### **2. Reddit API (if you want Reddit data):**
```bash
# Get credentials from: https://www.reddit.com/prefs/apps
# Add to .env:
echo "REDDIT_CLIENT_ID=your_id" >> .env
echo "REDDIT_CLIENT_SECRET=your_secret" >> .env
# Then uncomment reddit-startups in config
```

---

## ğŸ“ **Where Data Goes**

All data saved to: `data/raw/`

```
data/raw/
â”œâ”€â”€ competitive/     # Startup intelligence (Hacker News, RSS, etc.)
â”œâ”€â”€ marketing/       # Marketing datasets
â”œâ”€â”€ pitch/           # Pitch examples, investor blogs
â”œâ”€â”€ ip_legal/        # Legal/IP data
â”œâ”€â”€ policy/          # Policy documents
â””â”€â”€ team/            # Job postings, hiring guides
```

---

## ğŸ¯ **Quick Reference**

### **Main Script:**
```bash
./venv/bin/python scripts/download_datasets.py
```

### **Config File:**
`scripts/config/dataset_config.yaml`

### **Skip Existing Data:**
âœ… Automatic! No flags needed.

### **Force Re-download:**
Delete the directory/file first, then run:
```bash
rm -rf data/raw/competitive/hackernews/
./venv/bin/python scripts/download_datasets.py --agents competitive
```

---

## ğŸ“‹ **What Gets Downloaded**

### **Competitive (Startup Intelligence):**
- Hacker News stories (13+ startup-related)
- TechCrunch RSS articles
- VentureBeat RSS articles
- YC Library articles
- Startup blogs (YC, Paul Graham)
- Indie Hackers content
- News datasets (BBC, AG News)
- Kaggle startup datasets

### **Pitch:**
- Pitch examples
- Investor blogs (a16z, Sequoia, First Round)
- Startup failure post-mortems
- YC pitch templates
- Podcast pages

### **Marketing:**
- Ad creative examples
- Review datasets (IMDB, Yelp, Amazon)

### **IP/Legal:**
- Privacy QA datasets
- OSS policies
- Patent guides

### **Policy:**
- Privacy compliance data
- Historical privacy policies

### **Team:**
- Job postings
- Job skill sets

---

## âœ… **That's It!**

Just run:
```bash
./venv/bin/python scripts/download_datasets.py
```

The script will:
1. âœ… Skip existing data automatically
2. âœ… Download only what's missing
3. âœ… Show progress with âœ“ or âœ—
4. âœ… Print summary at the end

**Safe to run multiple times!** It won't re-download what you already have.

